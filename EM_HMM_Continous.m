function [ model ] = EM_HMM_Continous(X,pi,A,B,Max_iter)
%%  perform the EM algorithm to estimate the parameters. 
%input:
%   X:  set of observations generated by the HMM Generate Data functions 
%   pi: probability  hidden state.
%   B: Emmision Probability Matrix
%   A: transition matrix
%return:
%    model.pi prpbility of hiddent variables.
%    model.A new transmission matrix 
%    model.B emmision matrix
%    model.loglike the final loglikelihood of evaluation.
[N,T] = size(X);% N: number of sequences T: length of sequences
[K,~]=size(pi); %K: hidden state numbers

% random initialization
pi = rand(K,1)+eps; 
pi = pi ./sum(pi);
epsilon=10e-6 % convergence

%initialise A,E
A= rand(K,K)+eps; 
A = bsxfun(@times,A,1./sum(A,2));%transmission matrix

E.mu=rand(size(B.mu));
E.sigma2= rand(size(B.sigma2));% emmission matrix

loglike=zeros(1,Max_iter);
for iteration = 1:Max_iter 
    iteration
   %initialise
    pi_new =0 .* pi;
    A_new = 0 .* A;
    E_new.mu =zeros(size(B.mu));
    E_new.sigma2= zeros(size(B.sigma2));
    sum_gamma = zeros(K,1);
    
    %loop all the samples X1:N
    for index = 1:N
       % generate gaussian model
        Emission_matrix= zeros(K,T);
        for k=1:K %% generate k gaussian distributions for hidden state density
%             if isnan(E.sigma2(k))
%                E.sigma2= rand(size(B.sigma2));
%             end 
            Emission_matrix(k,:) = mvnpdf(X(index,:)',E.mu(k),E.sigma2(k));
        end
        %%Emission_matrix=bsxfun(@times,Emission_matrix,1./sum(Emission_matrix,2));
       
        %% E-step: calculate expectations

       %calculation back- forward
       [alpha,scale]=Forwards(A,Emission_matrix,pi,K,T);
        beta=backwards(A,Emission_matrix,K,T,scale);
        
%     
          gamma=alpha.*beta;% smooth posterior p(zt|x1:T)
          gamma = bsxfun(@times,gamma,1./sum(gamma));
          
          %two-slice joint smooth posterior
          %%eps_joint=joint_prob(X,A,Emission_matrix,alpha, beta,index);
          
           % compute expectation of joint posterior of states at t,t+1. 
            joint = A .* 0;
            for t = 1:T-1
                temp = (alpha(:,t) * (beta(:,t+1) .* Emission_matrix(:,t+1))' ).* A;
                joint= joint + temp/ sum(sum(temp));
            end
            
        %% M-step update parameters to maximise
          pi_new = pi_new + gamma(:,1);
          A_new = A_new +joint;        
          E_new.mu =   E_new.mu + X(index,:)*gamma';
          for k = 1:K
          E_new.sigma2(:,k) =  E_new.sigma2(:,k) + bsxfun(@times,X(index,:),gamma(k,:))* X(index,:)'+1;
          end
          
          sum_gamma = sum_gamma + sum(gamma,2);
           % evaluate log-likelihood
          loglike(iteration) = loglike(iteration) - sum(log(scale));  
        


    end
          
          % updates
          pi= sum(A_new,2);
          pi = pi/ sum(pi);
          A = bsxfun(@times,A_new,1./sum(A_new,2));
          E.mu = bsxfun(@times, E_new.mu ,1./sum_gamma');
          E.sigma2= bsxfun(@times,E_new.sigma2,1./reshape(sum_gamma,1,K));
          
          for k= 1:K
               E.sigma2(:,k) =  E.sigma2(:,k) -  E.mu(:,k)* E.mu(:,k)';
               E.sigma2(:,k)= ( E.sigma2(:,k) +  E.sigma2(:,k)')/ 2;
          end
        
            % check if it converges - stops changing
           
            
          if  iteration>=2 
              convergence= abs(1-loglike(iteration-1)*1.0/loglike(iteration));
              if convergence<epsilon
                  break;
              end 
          end     
    
       

    
end
    model.pi = pi;
    model.A = A;
    model.mu=E.mu;
    model.sigma2=E.sigma2;
    model.loglike=loglike(iteration);
end



function [alpha,scale]=Forwards(A,emission,pi,K,T)
%%Forward - alpha 
  % compute recursively the posterior density p(zt|x1:t) 
  %input:
  % A transmission matrix 
  % B emission probability vector
  % X observation
  % pi initial state distribution
  % i: sample index,i th sample among the N observations
  % K: state numbers
  % T: time length.
  %output: 
  %alpha
  %Z:log probablity of the evidence logp(X 1:T)
   alpha=zeros(K,T);
  % initial step:
   b1=emission(:,1); %% emission probility for sample i at time 1
   alpha(:,1)= pi.*b1; %%p()
   scale(1)=1./sum(alpha(:,1));
   %normalise
   alpha(:,1) = alpha(:,1)/scale(1);
   
   % predict-update cycle.
   for t = 2:T
        bt=emission(:,t);
        alpha(:,t)=bt.*(A'* alpha(:,t-1));
        scale(t)=1./sum(alpha(:,t));
        %belief state at time t
        alpha(:,t) = alpha(:,t)*scale(t); %%normalise       
        %logp_X_T=logp_X_T+log(Z(t));          
   end 
 
end

function [beta]=backwards(A,emission,K,T,scale_alpha)
%%backwards - beta 
  % compute recursively the conditonal likelihood of future evidence given
  % that the hidden state at time t is j
  %input:
  % A transmission matrix 
  % B emission probability vector
  % X observations
  % pi initial state distribution
  % i: sample index,i th sample among the N observations
  % K: state numbers
  % T: time length.
  %output: 
  %beta
 % initial step:
   beta=ones(K,T); 
   % step-back cycle.
   for t= T-1:-1:1
        bt=emission(:,t+1);%% next time emission probablity
        beta(:,t)=A*(bt.*beta(:,t+1)); 
        % in case of underflow 
        beta(:,t)=beta(:,t)*scale_alpha(t);
   end 
 
end

